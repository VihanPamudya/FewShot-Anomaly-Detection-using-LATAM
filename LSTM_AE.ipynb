{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "from math import sqrt, floor\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import r2_score, make_scorer, accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "import tensorflow as tf\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import array\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import optimizers, Sequential\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "\n",
    "path_errors = 'datapath'\n",
    "path_features = 'datapath'\n",
    "df_errors = pd.read_csv(path_errors)\n",
    "df_features = pd.read_csv(path_features)\n",
    "total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "tot_feat = pd.read_csv('datapath')\n",
    "tot_feat = list(tot_feat['feat'])\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "\n",
    "\n",
    "def create_lstm_autoencoder_model(layer1=20, layer2=6, lr=0.01):\n",
    "    lstm_autoencoder = Sequential()\n",
    "    lstm_autoencoder.add(LSTM(layer1, activation='relu', input_shape=(8, 137), return_sequences=True))\n",
    "    lstm_autoencoder.add(LSTM(layer2, activation='relu', return_sequences=False))\n",
    "    lstm_autoencoder.add(RepeatVector(8))\n",
    "    lstm_autoencoder.add(LSTM(layer2, activation='relu', return_sequences=True))\n",
    "    lstm_autoencoder.add(LSTM(layer1, activation='relu', return_sequences=True))\n",
    "    lstm_autoencoder.add(TimeDistributed(Dense(137)))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr)\n",
    "    lstm_autoencoder.compile(loss='mse', optimizer=adam)    \n",
    "    return lstm_autoencoder\n",
    "\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    print(param_dict)\n",
    "    return param_dict['model__batch_size'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2']\n",
    "\n",
    "callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "dayWiseResultsTest = {}\n",
    "dayWiseResultsTrain = {}\n",
    "lookback = 8\n",
    "for itr in range(1,4):\n",
    "    RESULTS_PATH_TRAIN = f'datapath'\n",
    "    RESULTS_PATH_TEST = f'datapath'\n",
    "    for dayStart in [14, 30, 90]:\n",
    "        hyp = '2W' if dayStart == 14 else '1M' if dayStart == 30 else '3M' \n",
    "        df_hyp = pd.read_csv(f\"datapath/LstmAE{hyp}.csv\")\n",
    "        print(dayStart)\n",
    "        daysrange = str(dayStart)\n",
    "        source_dict_results = {}\n",
    "        source_dict_results_train = {}\n",
    "        lst=[]\n",
    "        for i in range(19):\n",
    "            tot_feat.append('ErrBits')\n",
    "            print(i)\n",
    "            inverter = i\n",
    "            df_inv_0 = pd.read_csv(f'datapath/{inverter}.csv')\n",
    "            df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "            df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "            df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "\n",
    "            format = '%Y-%m-%d %H:%M:%S'\n",
    "            error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "            start_days = 30+dayStart\n",
    "            start_date = error_date-timedelta(days= start_days)\n",
    "            end_date = error_date+timedelta(days= 1*30)\n",
    "            split_date = error_date-timedelta(days= 1*30)\n",
    "            df_inv_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index < end_date)]\n",
    "            df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "            for feat in tot_feat:\n",
    "                if feat not in df_inv_0.columns:\n",
    "                    df_inv_0[feat] = 0\n",
    "            df_inv_0 = df_inv_0[tot_feat]\n",
    "            df_inv_0 = df_inv_0.dropna()\n",
    "\n",
    "            df_target = df_inv_0[['ErrBits']]\n",
    "            tot_feat.remove('ErrBits')\n",
    "            df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "            Xtrain_0, Xtest_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "            print(error_date, start_date, split_date, end_date)\n",
    "            print(df_inv_0.shape, Xtrain_0.shape, Xtest_0.shape)\n",
    "            Timestamp_train_0, Timestamp_test_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)].index, df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)].index\n",
    "            ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "            ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "            print(Xtrain_0.shape)\n",
    "            if Xtrain_0.shape[0]>1:\n",
    "                X_scaler = MinMaxScaler()\n",
    "                X_scaler.fit(Xtrain_0)\n",
    "                xtrain = X_scaler.transform(Xtrain_0)\n",
    "                xtest = X_scaler.transform(Xtest_0)\n",
    "                xtrain = temporalize(xtrain)\n",
    "                xtest = temporalize(xtest)\n",
    "                model__batch_size, model__epochs, model__layer1, model__layer2 = getHyperParams(inverter, df_hyp)\n",
    "                model  = create_lstm_autoencoder_model(layer1=model__layer1, layer2=model__layer2)\n",
    "                model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=False,validation_split=0.1, callbacks=callback1)\n",
    "                test_enc = model.predict(xtest)\n",
    "                train_enc = model.predict(xtrain)\n",
    "                test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "                train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "                test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "                train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "\n",
    "                mse_threshold = np.max(train_mses)\n",
    "                mae_threshold = np.max(train_maes)\n",
    "\n",
    "                ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "                ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "\n",
    "\n",
    "                test_df = pd.DataFrame(ytest)\n",
    "                test_df['mse_scores'] = test_mses\n",
    "                test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "                test_df['mae_scores'] = test_maes\n",
    "                test_df['ypred_mse'] = ypred_mse\n",
    "                test_df['ypred_mae'] = ypred_mae\n",
    "                lst.append(f1_score(y_true=test_df['ytrue'], y_pred=test_df['ypred_mse']))\n",
    "                test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "                source_dict_results[f'inv_{i}'] = test_df\n",
    "                \n",
    "                train_df = pd.DataFrame(ytrain)\n",
    "                train_df['mse_scores'] = train_mses\n",
    "                train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "                train_df['mae_scores'] = train_maes\n",
    "                train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "                source_dict_results_train[f'inv_{i}'] = train_df\n",
    "                \n",
    "        dayWiseResultsTest[dayStart] = source_dict_results \n",
    "        dayWiseResultsTrain[dayStart] = source_dict_results_train\n",
    "        print(lst)\n",
    "    with open(f'{RESULTS_PATH_TRAIN}/train_STL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(dayWiseResultsTrain, f) \n",
    "    with open(f'{RESULTS_PATH_TEST}/test_STL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(dayWiseResultsTest, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "799209b20b2b97cb7d521719e25d3ce7f86c9d675fdfb18aaaa1ffc1f301c415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
