{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 08:17:59.481727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:17:59.481767: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0\n",
      "2019-10-11 08:24:20 2019-08-28 08:24:20 2019-09-11 08:24:20 2019-11-10 08:24:20\n",
      "(14406, 137) (2761, 137) (11645, 137)\n",
      "(2761, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 08:19:38.628412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.628678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.628858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.629044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.665011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.665121: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-31 08:19:38.665135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-31 08:19:38.665575: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2019-04-02 15:20:48 2019-02-17 15:20:48 2019-03-03 15:20:48 2019-05-02 15:20:48\n",
      "(14115, 137) (2884, 137) (11231, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "2\n",
      "2018-12-28 09:46:38 2018-11-14 09:46:38 2018-11-28 09:46:38 2019-01-27 09:46:38\n",
      "(14142, 137) (2884, 137) (11258, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "3\n",
      "2018-10-13 16:26:39 2018-08-30 16:26:39 2018-09-13 16:26:39 2018-11-12 16:26:39\n",
      "(13338, 137) (2199, 137) (11139, 137)\n",
      "(2199, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "4\n",
      "2018-10-13 13:18:07 2018-08-30 13:18:07 2018-09-13 13:18:07 2018-11-12 13:18:07\n",
      "(13150, 137) (2225, 137) (10925, 137)\n",
      "(2225, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "5\n",
      "2019-05-19 09:01:02 2019-04-05 09:01:02 2019-04-19 09:01:02 2019-06-18 09:01:02\n",
      "(15044, 137) (2763, 137) (12281, 137)\n",
      "(2763, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "6\n",
      "2018-06-12 10:55:50 2018-04-29 10:55:50 2018-05-13 10:55:50 2018-07-12 10:55:50\n",
      "(13520, 137) (2655, 137) (10865, 137)\n",
      "(2655, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "7\n",
      "2019-05-27 04:57:13 2019-04-13 04:57:13 2019-04-27 04:57:13 2019-06-26 04:57:13\n",
      "(15203, 137) (2884, 137) (12319, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "8\n",
      "2019-09-21 15:53:48 2019-08-08 15:53:48 2019-08-22 15:53:48 2019-10-21 15:53:48\n",
      "(11485, 137) (2884, 137) (8601, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "9\n",
      "2020-01-19 07:22:19 2019-12-06 07:22:19 2019-12-20 07:22:19 2020-02-18 07:22:19\n",
      "(15030, 137) (2884, 137) (12146, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "10\n",
      "2019-04-10 12:17:11 2019-02-25 12:17:11 2019-03-11 12:17:11 2019-05-10 12:17:11\n",
      "(14737, 137) (2884, 137) (11853, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "11\n",
      "2019-08-18 12:36:11 2019-07-05 12:36:11 2019-07-19 12:36:11 2019-09-17 12:36:11\n",
      "(14636, 137) (2884, 137) (11752, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "12\n",
      "2019-08-24 13:13:44 2019-07-11 13:13:44 2019-07-25 13:13:44 2019-09-23 13:13:44\n",
      "(14910, 137) (2674, 137) (12236, 137)\n",
      "(2674, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 32}\n",
      "13\n",
      "2019-11-05 11:51:23 2019-09-22 11:51:23 2019-10-06 11:51:23 2019-12-05 11:51:23\n",
      "(15006, 137) (2884, 137) (12122, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "14\n",
      "2019-10-12 15:00:20 2019-08-29 15:00:20 2019-09-12 15:00:20 2019-11-11 15:00:20\n",
      "(15223, 137) (2884, 137) (12339, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "15\n",
      "2019-10-13 09:51:49 2019-08-30 09:51:49 2019-09-13 09:51:49 2019-11-12 09:51:49\n",
      "(15217, 137) (2884, 137) (12333, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "16\n",
      "2019-10-26 08:15:14 2019-09-12 08:15:14 2019-09-26 08:15:14 2019-11-25 08:15:14\n",
      "(15224, 137) (2884, 137) (12340, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "17\n",
      "2019-10-18 07:26:00 2019-09-04 07:26:00 2019-09-18 07:26:00 2019-11-17 07:26:00\n",
      "(15223, 137) (2884, 137) (12339, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "18\n",
      "2020-05-23 13:56:28 2020-04-09 13:56:28 2020-04-23 13:56:28 2020-06-22 13:56:28\n",
      "(15194, 137) (2884, 137) (12310, 137)\n",
      "(2884, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "[0.2353946993445426, 0.14532429320711268, 0.4349306738019946, 0.7029465399750456, 0.5359149037886937, 0.25645438898450945, 0.03078358208955224, 0.10960134755755195, 0.42952602952602953, 0.685212033345415, 0.0656896100497252, 0.8168973678548376, 0.6561053755784977, 0.3820203892493049, 0.23460935751063353, 0.012071156289707752, 0.007862959842740803, 0.02016724053123463, 0.43944113197472634]\n",
      "30\n",
      "0\n",
      "2019-10-11 08:24:20 2019-08-12 08:24:20 2019-09-11 08:24:20 2019-11-10 08:24:20\n",
      "(17702, 137) (6057, 137) (11645, 137)\n",
      "(6057, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "1\n",
      "2019-04-02 15:20:48 2019-02-01 15:20:48 2019-03-03 15:20:48 2019-05-02 15:20:48\n",
      "(17293, 137) (6062, 137) (11231, 137)\n",
      "(6062, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n",
      "2\n",
      "2018-12-28 09:46:38 2018-10-29 09:46:38 2018-11-28 09:46:38 2019-01-27 09:46:38\n",
      "(17438, 137) (6180, 137) (11258, 137)\n",
      "(6180, 137)\n",
      "{'model__batch_size': 128, 'model__epochs': 200, 'model__filter1': 128, 'model__filter2': 64}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "from math import sqrt, floor\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import r2_score, make_scorer, accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "import tensorflow as tf\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import array\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import optimizers, Sequential\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import ast\n",
    "from keras.layers import Attention, GlobalMaxPooling1D, AveragePooling1D, UpSampling1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tcn import TCN\n",
    "\n",
    "\n",
    "path_errors = 'datapath'\n",
    "path_features = 'datapath'\n",
    "df_errors = pd.read_csv(path_errors)\n",
    "df_features = pd.read_csv(path_features)\n",
    "total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "tot_feat = pd.read_csv('datapath')\n",
    "tot_feat = list(tot_feat['feat'])\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "\n",
    "\n",
    "def create_tcn_model(lookback=8, ts_dims = 137, filter1=40, filter2=6, sampling_factor = 4, lr = 0.001):\n",
    "    model = Sequential([\n",
    "    TCN(input_shape=(lookback, ts_dims), nb_filters=filter1, kernel_size=2, dilations=(1,2,4,8,16), \n",
    "                      padding='same', use_skip_connections=True, dropout_rate=0.00, return_sequences=True,\n",
    "                       name='tcn-enc'),\n",
    "    \n",
    "    Conv1D(filters=filter2, kernel_size=1, activation='linear', padding='same'),\n",
    "    \n",
    "    AveragePooling1D(pool_size=sampling_factor, strides=None, padding='valid'),\n",
    "    \n",
    "    Activation(\"linear\"),\n",
    "    \n",
    "    UpSampling1D(size=sampling_factor),\n",
    "    \n",
    "    TCN(nb_filters=filter1, kernel_size=2, dilations=(1,2,4,8,16), \n",
    "                      padding='same', use_skip_connections=True, dropout_rate=0.00, return_sequences=True,\n",
    "                    name='tcn-dec'),\n",
    "    \n",
    "    Dense(ts_dims, activation='linear')\n",
    "    \n",
    "])\n",
    "    adam = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=adam)    \n",
    "\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    print(param_dict)\n",
    "    return param_dict['model__batch_size'], param_dict['model__epochs'],param_dict['model__filter1'], param_dict['model__filter2']\n",
    "\n",
    "callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "dayWiseResultsTest = {}\n",
    "dayWiseResultsTrain = {}\n",
    "lookback = 8\n",
    "for itr in range(1,4):\n",
    "    RESULTS_PATH_TRAIN = f'datapath'\n",
    "    RESULTS_PATH_TEST = f'datapath'\n",
    "    for dayStart in [14, 30, 90]:\n",
    "        hyp = '2W' if dayStart == 14 else '1M' if dayStart == 30 else '3M' \n",
    "        df_hyp = pd.read_csv(f\"datapath/TcnAE{hyp}.csv\")\n",
    "        print(dayStart)\n",
    "        daysrange = str(dayStart)\n",
    "        source_dict_results = {}\n",
    "        source_dict_results_train = {}\n",
    "        lst=[]\n",
    "        for i in range(19):\n",
    "            tot_feat.append('ErrBits')\n",
    "            print(i)\n",
    "            inverter = i\n",
    "            df_inv_0 = pd.read_csv(f'datapath/{inverter}.csv')\n",
    "            df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "            df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "            df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "\n",
    "            format = '%Y-%m-%d %H:%M:%S'\n",
    "            error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "            start_days = 30+dayStart\n",
    "            start_date = error_date-timedelta(days= start_days)\n",
    "            end_date = error_date+timedelta(days= 1*30)\n",
    "            split_date = error_date-timedelta(days= 1*30)\n",
    "            df_inv_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index < end_date)]\n",
    "            df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "            for feat in tot_feat:\n",
    "                if feat not in df_inv_0.columns:\n",
    "                    df_inv_0[feat] = 0\n",
    "            df_inv_0 = df_inv_0[tot_feat]\n",
    "            df_inv_0 = df_inv_0.dropna()\n",
    "\n",
    "            df_target = df_inv_0[['ErrBits']]\n",
    "            tot_feat.remove('ErrBits')\n",
    "            df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "            Xtrain_0, Xtest_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "            print(error_date, start_date, split_date, end_date)\n",
    "            print(df_inv_0.shape, Xtrain_0.shape, Xtest_0.shape)\n",
    "            Timestamp_train_0, Timestamp_test_0 = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)].index, df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)].index\n",
    "            ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "            ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "            print(Xtrain_0.shape)\n",
    "            if Xtrain_0.shape[0]>1:\n",
    "                X_scaler = MinMaxScaler()\n",
    "                X_scaler.fit(Xtrain_0)\n",
    "                xtrain = X_scaler.transform(Xtrain_0)\n",
    "                xtest = X_scaler.transform(Xtest_0)\n",
    "                xtrain = temporalize(xtrain)\n",
    "                xtest = temporalize(xtest)\n",
    "                model__batch_size, model__epochs, model__filter1, model__filter2 = getHyperParams(inverter, df_hyp)\n",
    "                model  = create_tcn_model(filter1=model__filter1, filter2=model__filter2)\n",
    "                model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=False,validation_split=0.1, callbacks=callback1)\n",
    "                test_enc = model.predict(xtest)\n",
    "                train_enc = model.predict(xtrain)\n",
    "                test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "                train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "                test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "                train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "\n",
    "                mse_threshold = np.max(train_mses)\n",
    "                mae_threshold = np.max(train_maes)\n",
    "\n",
    "                ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "                ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "\n",
    "\n",
    "                test_df = pd.DataFrame(ytest)\n",
    "                test_df['mse_scores'] = test_mses\n",
    "                test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "                test_df['mae_scores'] = test_maes\n",
    "                test_df['ypred_mse'] = ypred_mse\n",
    "                test_df['ypred_mae'] = ypred_mae\n",
    "                lst.append(f1_score(y_true=test_df['ytrue'], y_pred=test_df['ypred_mse']))\n",
    "                test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "                source_dict_results[f'inv_{i}'] = test_df\n",
    "                \n",
    "                train_df = pd.DataFrame(ytrain)\n",
    "                train_df['mse_scores'] = train_mses\n",
    "                train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "                train_df['mae_scores'] = train_maes\n",
    "                train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "                source_dict_results_train[f'inv_{i}'] = train_df\n",
    "                \n",
    "        dayWiseResultsTest[dayStart] = source_dict_results \n",
    "        dayWiseResultsTrain[dayStart] = source_dict_results_train\n",
    "        print(lst)\n",
    "    with open(f'{RESULTS_PATH_TRAIN}/train_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(dayWiseResultsTrain, f) \n",
    "    with open(f'{RESULTS_PATH_TEST}/test_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(dayWiseResultsTest, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "799209b20b2b97cb7d521719e25d3ce7f86c9d675fdfb18aaaa1ffc1f301c415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
